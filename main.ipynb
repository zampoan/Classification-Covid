{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dir = \"covid-dataset\"\n",
    "class_names = ['COVID', 'Normal', 'Viral Pneumonia']\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Make directories with each class name\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Train and test ratio\n",
    "split_ratio = 0.8\n",
    "covid_train_split = int(len(os.listdir(\"covid-dataset/COVID/images/\")) * split_ratio)\n",
    "normal_train_split = int(len(os.listdir(\"covid-dataset/Normal/images/\")) * split_ratio)\n",
    "pneumonia_train_split = int(len(os.listdir(\"covid-dataset/Viral Pneumonia/images/\")) * split_ratio)\n",
    "\n",
    "# Shuffle directories\n",
    "for class_name in class_names:\n",
    "    random.shuffle(os.listdir(dataset_dir + \"/\" + class_name + \"/images\"))\n",
    "\n",
    "# Move images into train and test\n",
    "covid_src_images = os.listdir(\"covid-dataset/COVID/images/\")\n",
    "train_covid_images = covid_src_images[:covid_train_split]\n",
    "test_covid_images = covid_src_images[covid_train_split:]\n",
    "for image in train_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/COVID\")\n",
    "for image in test_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/COVID\")\n",
    "\n",
    "normal_src_images = os.listdir(\"covid-dataset/Normal/images/\")\n",
    "train_normal_images = normal_src_images[:normal_train_split]\n",
    "test_normal_images = normal_src_images[normal_train_split:]\n",
    "for image in train_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Normal\")\n",
    "for image in test_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Normal\")\n",
    "\n",
    "pneumonia_src_images = os.listdir(\"covid-dataset/Viral Pneumonia/images/\")\n",
    "train_pneumonia_images = pneumonia_src_images[:pneumonia_train_split]\n",
    "test_pneumonia_images = pneumonia_src_images[pneumonia_train_split:]\n",
    "for image in train_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Viral Pneumonia\")\n",
    "for image in test_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Viral Pneumonia\")\n",
    "\n",
    "print(\"Files moved successfully!\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'Normal', 'Viral Pneumonia']\n",
      "Train data: Dataset ImageFolder\n",
      "    Number of datapoints: 12121\n",
      "    Root location: covid-dataset\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) \n",
      " Test data: Dataset ImageFolder\n",
      "    Number of datapoints: 3032\n",
      "    Root location: covid-dataset\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f\"Train data: {train_data} \\n Test data: {test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Turn it into a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scripts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/data_setup.py\n",
    "\"\"\"\n",
    "Putting data into Imagefolder and Dataloader\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "def create_dataloaders(train_dir, test_dir, transform, batch_size, num_workers):\n",
    "    train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_data,batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model (CovidAid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image.shape: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m img_single, label_single \u001b[39m=\u001b[39m img_batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label_batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSingle image.shape: \u001b[39m\u001b[39m{\u001b[39;00mimg_single\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m model_0\u001b[39m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m     11\u001b[0m     pred \u001b[39m=\u001b[39m model_0(img_single\u001b[39m.\u001b[39mto(device))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_0' is not defined"
     ]
    }
   ],
   "source": [
    "# Dummy forward pass to test if it works\n",
    "# batches of images and its label\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# testibg model to see if it works\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image.shape: {img_single.shape}\")\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_0(img_single.to(device))\n",
    "    \n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 CovidAid Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/covid_aid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/covid_aid.py\n",
    "\"\"\"\n",
    "Contains code about CovidAid Model. Original paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9418407\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Train and Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "        break\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model_0, train_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Convert to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/training.py\n",
    "import torch\n",
    "import torchmetrics\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Convert to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/save_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/save_model.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    target_dir = os.path.join(\"../covid-dataset\", target_dir)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train, Evaluate & Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m     29\u001b[0m \u001b[39m# Train models\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model_0_results \u001b[39m=\u001b[39m train(model_0, optimizer, loss_fn, EPOCHS, device)\n\u001b[0;32m     32\u001b[0m \u001b[39m# End timer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS= 4 #os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# MODELS\n",
    "model_0 = CovidAidModel().to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy_fn = MulticlassAccuracy(num_classes=3)\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Start timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train models\n",
    "model_0_results = train(model_0, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "# End timer\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "# Save Model\n",
    "save_model(model_0, target_dir='models', model_name='CovidAid.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Convert to Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/executable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/executable.py\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, covid_aid, save_model, training\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 5\n",
    "print(f\"Learning Rate: {LEARNING_RATE} | Number of Epochs: {EPOCHS}\")\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# DATA\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "# MODELS\n",
    "model_0 = covid_aid.CovidAidModel().to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Start timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train models\n",
    "model_0_results = training.train(model_0, train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "# End timer\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "# Save Model\n",
    "save_model.save_model(model_0, target_dir='models', model_name='CovidAid.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
