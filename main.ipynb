{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'covid-dataset/COVID/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Train and test ratio\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m split_ratio \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m covid_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39mcovid-dataset/COVID/images/\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m*\u001b[39m split_ratio)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m normal_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mcovid-dataset/Normal/images/\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m*\u001b[39m split_ratio)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pneumonia_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mcovid-dataset/Viral Pneumonia/images/\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m*\u001b[39m split_ratio)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'covid-dataset/COVID/images/'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dir = \"covid-dataset\"\n",
    "class_names = ['COVID', 'Normal', 'Viral Pneumonia']\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Make directories with each class name\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Train and test ratio\n",
    "split_ratio = 0.8\n",
    "covid_train_split = int(len(os.listdir(\"covid-dataset/COVID/images/\")) * split_ratio)\n",
    "normal_train_split = int(len(os.listdir(\"covid-dataset/Normal/images/\")) * split_ratio)\n",
    "pneumonia_train_split = int(len(os.listdir(\"covid-dataset/Viral Pneumonia/images/\")) * split_ratio)\n",
    "\n",
    "# Shuffle directories\n",
    "for class_name in class_names:\n",
    "    random.shuffle(os.listdir(dataset_dir + \"/\" + class_name + \"/images\"))\n",
    "\n",
    "# Move images into train and test\n",
    "covid_src_images = os.listdir(\"covid-dataset/COVID/images/\")\n",
    "train_covid_images = covid_src_images[:covid_train_split]\n",
    "test_covid_images = covid_src_images[covid_train_split:]\n",
    "for image in train_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/COVID\")\n",
    "for image in test_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/COVID\")\n",
    "\n",
    "normal_src_images = os.listdir(\"covid-dataset/Normal/images/\")\n",
    "train_normal_images = normal_src_images[:normal_train_split]\n",
    "test_normal_images = normal_src_images[normal_train_split:]\n",
    "for image in train_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Normal\")\n",
    "for image in test_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Normal\")\n",
    "\n",
    "pneumonia_src_images = os.listdir(\"covid-dataset/Viral Pneumonia/images/\")\n",
    "train_pneumonia_images = pneumonia_src_images[:pneumonia_train_split]\n",
    "test_pneumonia_images = pneumonia_src_images[pneumonia_train_split:]\n",
    "for image in train_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Viral Pneumonia\")\n",
    "for image in test_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Viral Pneumonia\")\n",
    "\n",
    "print(\"Files moved successfully!\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'Normal', 'Viral Pneumonia']\n",
      "Train data: Dataset ImageFolder\n",
      "    Number of datapoints: 12121\n",
      "    Root location: covid-dataset/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) \n",
      " Test data: Dataset ImageFolder\n",
      "    Number of datapoints: 3032\n",
      "    Root location: covid-dataset/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f\"Train data: {train_data} \\n Test data: {test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Turn it into a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scripts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/data_setup.py\n",
    "\"\"\"\n",
    "Putting data into Imagefolder and Dataloader\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "def create_dataloaders(train_dir, test_dir, transform, batch_size, num_workers):\n",
    "    train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_data,batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Model (CovidAid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x) # [1, 3, 11, 11]\n",
    "        x = self.flatten(x) # [1, 363]\n",
    "        x = self.linear(x) # [1,3]\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image.shape: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Dummy forward pass to test if it works\n",
    "# batches of images and its label\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# testibg model to see if it works\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image.shape: {img_single.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n",
      "tensor([[0.4324, 0.2526, 0.3150]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_z/j5d8xzdn6fg4qqht33ql5qdc0000gn/T/ipykernel_3866/1711460522.py:120: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "# test_img = torch.randn(32,3,256,256)\n",
    "# print(test_img.shape)\n",
    "model = CovidAidModel()\n",
    "print(f\"Output shape: {model(img_single).shape}\")\n",
    "print(model(img_single))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 CovidAid Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/covid_aid.py\n",
    "\"\"\"\n",
    "Contains code about CovidAid Model. Original paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9418407\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x) # [32,3]\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Squeeze Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Squeeze Net takes input of 224 instead of 256, so have to scale it accordingly\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    \"\"\"\n",
    "    s1x1: number of filters in squeeze layer (all 1x1)\n",
    "    e1x1: number of 1x1 filters in expand layer\n",
    "    e3x3: number of 3x3 filters in expand layer\n",
    "    s1x1 > (e1x1 + e3x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, s1x1,e1x1,e3x3,):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=s1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.expand_e1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.expand_e3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        y = self.expand_e1x1(x) # 64\n",
    "        z = self.expand_e3x3(x) # 64\n",
    "        return torch.cat((y, z),dim=1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=0)\n",
    "        self.conv_10 = nn.Conv2d(in_channels=512, out_channels=3, kernel_size=1, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.maxpool_8 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.fireModule_2 = FireModule(96, 16, 64, 64)\n",
    "        self.fireModule_3 = FireModule(128, 16, 64, 64)\n",
    "        self.fireModule_4 = FireModule(128, 32, 128, 128)\n",
    "        self.fireModule_5 = FireModule(256, 32, 128, 128)\n",
    "        self.fireModule_6 = FireModule(256, 48, 192, 192)\n",
    "        self.fireModule_7 = FireModule(384, 48, 192, 192)   \n",
    "        self.fireModule_8 = FireModule(384, 64, 256, 256)\n",
    "        self.fireModule_9 = FireModule(512, 64, 256, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.avgpool_10 = nn.AvgPool2d((1,1))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(867, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x) # [32, 96, 109, 109]\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool_1(x) # [32, 96, 54, 54]\n",
    "        x = self.fireModule_2(x) # [32, 128,55,55]\n",
    "        x = self.fireModule_3(x) # [128,55,55]\n",
    "        x = self.fireModule_4(x) # [256,55,55]\n",
    "        x = self.maxpool_4(x) # [256,27,27]\n",
    "        x = self.fireModule_5(x) # [256,27,27]\n",
    "        x = self.fireModule_6(x) # [384, 27, 27]\n",
    "        x = self.fireModule_7(x) # [384, 27, 27]\n",
    "        x = self.fireModule_8(x) # [512, 27, 27]\n",
    "        x = self.maxpool_8(x) # [512, 13, 13]\n",
    "        x = self.fireModule_9(x) # [512, 13, 13]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv_10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool_10(x) # [3,1,1] \n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NNPACK SpatialConvolution_updateOutput failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m32\u001b[39m,\u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m SqueezeNet()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOutput shape: \u001b[39m\u001b[39m{\u001b[39;00mmodel(test_img)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# model(test_img)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool_8(x) \u001b[39m# [512, 13, 13]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfireModule_9(x) \u001b[39m# [512, 13, 13]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_10(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# x = self.relu(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# x = self.avgpool_10(x) # [3,1,1] \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# x = self.flatten(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# x = self.linear(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# x = self.softmax(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhengyaosiah/Documents/Code/Classification-Covid/main.ipynb#X24sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NNPACK SpatialConvolution_updateOutput failed"
     ]
    }
   ],
   "source": [
    "# model = SqueezeNet()\n",
    "# print(f\"Output shape: {model(img_single).shape}\")\n",
    "# print(model(img_single))\n",
    "\n",
    "# Below throws an error for some reason\n",
    "test_img = torch.rand(32,3, 224, 224)\n",
    "\n",
    "model = SqueezeNet()\n",
    "print(f\"Output shape: {model(test_img).shape}\")\n",
    "# model(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 SqueezeNet Script Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/squeeze_net.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/squeeze_net.py\n",
    "\"\"\"\"\n",
    "Squeeze Net takes input of 224 instead of 256, so have to scale it accordingly\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    \"\"\"\n",
    "    s1x1: number of filters in squeeze layer (all 1x1)\n",
    "    e1x1: number of 1x1 filters in expand layer\n",
    "    e3x3: number of 3x3 filters in expand layer\n",
    "    s1x1 > (e1x1 + e3x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, s1x1,e1x1,e3x3,):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=s1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.expand_e1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.expand_e3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        y = self.expand_e1x1(x) # 64\n",
    "        z = self.expand_e3x3(x) # 64\n",
    "        return torch.cat((y, z),dim=1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=0)\n",
    "        self.conv_10 = nn.Conv2d(in_channels=512, out_channels=3, kernel_size=1, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.maxpool_8 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        self.fireModule_2 = FireModule(96, 16, 64, 64)\n",
    "        self.fireModule_3 = FireModule(128, 16, 64, 64)\n",
    "        self.fireModule_4 = FireModule(128, 32, 128, 128)\n",
    "        self.fireModule_5 = FireModule(256, 32, 128, 128)\n",
    "        self.fireModule_6 = FireModule(256, 48, 192, 192)\n",
    "        self.fireModule_7 = FireModule(384, 48, 192, 192)   \n",
    "        self.fireModule_8 = FireModule(384, 64, 256, 256)\n",
    "        self.fireModule_9 = FireModule(512, 64, 256, 256)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.avgpool_10 = nn.AvgPool2d((1,1))\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(867, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x) # [32, 96, 109, 109]\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool_1(x) # [32, 96, 54, 54]\n",
    "        x = self.fireModule_2(x) # [32, 128,55,55]\n",
    "        x = self.fireModule_3(x) # [128,55,55]\n",
    "        x = self.fireModule_4(x) # [256,55,55]\n",
    "        x = self.maxpool_4(x) # [256,27,27]\n",
    "        x = self.fireModule_5(x) # [256,27,27]\n",
    "        x = self.fireModule_6(x) # [384, 27, 27]\n",
    "        x = self.fireModule_7(x) # [384, 27, 27]\n",
    "        x = self.fireModule_8(x) # [512, 27, 27]\n",
    "        x = self.maxpool_8(x) # [512, 13, 13]\n",
    "        x = self.fireModule_9(x) # [512, 13, 13]\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv_10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool_10(x) # [3,1,1] \n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Deep GRU-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep GRU-CNN input 224 x 224\n",
    "https://ieeexplore.ieee.org/abstract/document/9423965\n",
    "https://discuss.pytorch.org/t/input-shape-to-gru-layer/171318\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUCNN(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 64, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.gru_block = nn.Sequential(\n",
    "          nn.GRUCell(512*7*7, hidden_size),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(64),\n",
    "          nn.Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(64, num_classes), # 512*7*7*BATCH_SIZE\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.Dropout(p=0.5),\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(512*7*7, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x) # torch.Size([32, 512, 7, 7])\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # [32, 512 * 7 * 7]\n",
    "        x = self.gru_block(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n",
      "tensor([[0.3570, 0.3078, 0.3353]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengyaosiah/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "model = GRUCNN()\n",
    "print(f\"Output shape: {model(img_single).shape}\")\n",
    "print(model(img_single))\n",
    "\n",
    "# img_single = torch.randn([32, 3, 224, 224])\n",
    "# model = GRUCNN()\n",
    "# print(f\"Output shape: {model(img_single).shape}\")\n",
    "# model(img_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.1 GRU_CNN Script Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%writefile scripts/gru_cnn.py\n",
    "\"\"\"\n",
    "Deep GRU-CNN input 224 x 224\n",
    "https://ieeexplore.ieee.org/abstract/document/9423965\n",
    "https://discuss.pytorch.org/t/input-shape-to-gru-layer/171318\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUCNN(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 64, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.gru_block = nn.Sequential(\n",
    "          nn.GRUCell(512*7*7, hidden_size),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(64),\n",
    "          nn.Dropout(p=0.5)\n",
    "        )\n",
    "\n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(64, num_classes), # 512*7*7*BATCH_SIZE\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(3),\n",
    "            nn.Dropout(p=0.5),\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(512*7*7, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x) # torch.Size([32, 512, 7, 7])\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # [32, 512 * 7 * 7]\n",
    "        x = self.gru_block(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Efficient CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S1568494622007050#fig3\n",
    "Efficient_CNN: https://www.hindawi.com/journals/complexity/2021/6621607/fig4/\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EFFICIENT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1),\n",
    "            nn.MaxPool2d(1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        \n",
    "        self.dense_1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_3 = nn.Sequential(\n",
    "            nn.Linear(256, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        x = self.block_6(x) # [32, 256, 2, 2]\n",
    "        x = x.view(x.size(0), -1) #[32, 1024]\n",
    "\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = torch.randn(32,3,150, 150)\n",
    "\n",
    "model = EFFICIENT_CNN()\n",
    "print(f\"Output shape: {model(test_img).shape}\")\n",
    "print(model(test_img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4.1 Efficient_Cnn Script Mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/efficient_cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/efficient_cnn.py\n",
    "\"\"\"\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S1568494622007050#fig3\n",
    "Efficient_CNN: https://www.hindawi.com/journals/complexity/2021/6621607/fig4/\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EFFICIENT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1),\n",
    "            nn.MaxPool2d(1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        \n",
    "        self.dense_1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_3 = nn.Sequential(\n",
    "            nn.Linear(256, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        x = self.block_6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 CodnNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Focus(nn.Module): \n",
    "    # Focus wh information into c-space \n",
    "    def __init__(self, c1, c2, k=1, s=1, p=0, g=1):  # ch_in, ch_out, kernel, stride, padding, groups \n",
    "        super(Focus, self).__init__() \n",
    "        self.conv = nn.Conv2d(c1 * 4, c2, k, s, p, groups=g) \n",
    "        # self.contract = Contract(gain=2) \n",
    "\n",
    "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2) \n",
    "        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
    "\n",
    "class CodnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focus = Focus(3, 16)\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 28, 2, 2, groups=4), # kernel= 3x3?\n",
    "            nn.Conv2d(28, 28, 1),\n",
    "            nn.BatchNorm2d(28),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(44),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(44, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(60, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 1, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=2)\n",
    "        )\n",
    "        self.pool_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Hardswish(),\n",
    "            nn.AvgPool2d(8)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(96, 512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial Conv\n",
    "        x_cv = self.conv_1(x) # [32, 16, 128, 128\n",
    "\n",
    "        # Focus\n",
    "        x_f= self.focus(x) # [32, 16, 128, 628]\n",
    "\n",
    "        # Max Pool\n",
    "        x_mp = self.maxpool(x_cv) # [32, 16, 64, 64]\n",
    "\n",
    "        # Concat initial Conv with Focus\n",
    "        x_cc1 = torch.concat((x_cv, x_f), dim=1) #[32, 32, 128, 128]\n",
    "\n",
    "        # Depthwise conv 1\n",
    "        x_dcv1= self.depthwise_conv_1(x_cc1) # [32, 28, 64, 64]\n",
    "\n",
    "        # Avg Pool 1\n",
    "        x_ap1 = self.avgpool(x_dcv1) # [32, 28, 32, 32]\n",
    "\n",
    "        # Concat depwise conv with maxpool\n",
    "        x_cc2 = torch.concat((x_dcv1, x_mp), dim=1) # [ 32, 44, 64, 64]\n",
    "        \n",
    "        # Depthwise conv 2\n",
    "        x_dcv2 = self.depthwise_conv_2(x_cc2) #[32, 32, 32, 32]\n",
    "\n",
    "        # Avg Pool 2\n",
    "        x_ap2 = self.avgpool(x_dcv2) # [32, 32, 16, 16]\n",
    "\n",
    "        # Concat depthwise conv 2 with avgpool 1\n",
    "        x_cc3 = torch.concat((x_dcv2, x_ap1), dim=1) # [32, 60, 32, 32]\n",
    "\n",
    "        # Deptwise conv 3\n",
    "        x_dcv3 = self.depthwise_conv_3(x_cc3) # [32, 32, 16, 16]\n",
    "\n",
    "        # Avg Pool 3\n",
    "        x_ap3 = self.avgpool(x_dcv3) # [32,32,8,8]\n",
    "\n",
    "        # Concat depthwise conv3 with avgpool 2\n",
    "        x_cc4 = torch.concat((x_dcv3, x_ap2), dim=1) # [32, 64, 16, 16]\n",
    "\n",
    "        # Depthwise conv 4\n",
    "        x_dcv4 = self.depthwise_conv_4(x_cc4) # [32,32,8,8]\n",
    "\n",
    "        # Concat depwise conv4 with avgpool 3\n",
    "        x_cc5 = torch.concat((x_dcv4, x_ap3), dim=1) # [32, 64, 8, 8]\n",
    "\n",
    "        # Depwise conv 5\n",
    "        x_dcv5 = self.depthwise_conv_5(x_cc5) # [32, 32, 8, 8]\n",
    "\n",
    "        # Concat Depthwise conv4, AvgPool 3 and x_cc5\n",
    "        x = torch.cat((x_dcv4, x_ap3, x_dcv5), dim=1) # [32, 96, 8, 8]\n",
    "\n",
    "        x = self.pool_block(x) # [32, 96, 1, 1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/codnnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/codnnet.py\n",
    "\"\"\"\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S1568494622007050\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Focus(nn.Module): \n",
    "    # Focus wh information into c-space \n",
    "    def __init__(self, c1, c2, k=1, s=1, p=0, g=1):  # ch_in, ch_out, kernel, stride, padding, groups \n",
    "        super(Focus, self).__init__() \n",
    "        self.conv = nn.Conv2d(c1 * 4, c2, k, s, p, groups=g) \n",
    "        # self.contract = Contract(gain=2) \n",
    "\n",
    "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2) \n",
    "        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
    "\n",
    "class CodnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focus = Focus(3, 16)\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 28, 2, 2, groups=4), # kernel= 3x3?\n",
    "            nn.Conv2d(28, 28, 1),\n",
    "            nn.BatchNorm2d(28),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(44),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(44, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(60, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 1, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=2)\n",
    "        )\n",
    "        self.pool_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Hardswish(),\n",
    "            nn.AvgPool2d(8)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(96, 512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial Conv\n",
    "        x_cv = self.conv_1(x) # [32, 16, 128, 128\n",
    "\n",
    "        # Focus\n",
    "        x_f= self.focus(x) # [32, 16, 128, 628]\n",
    "\n",
    "        # Max Pool\n",
    "        x_mp = self.maxpool(x_cv) # [32, 16, 64, 64]\n",
    "\n",
    "        # Concat initial Conv with Focus\n",
    "        x_cc1 = torch.concat((x_cv, x_f), dim=1) #[32, 32, 128, 128]\n",
    "\n",
    "        # Depthwise conv 1\n",
    "        x_dcv1= self.depthwise_conv_1(x_cc1) # [32, 28, 64, 64]\n",
    "\n",
    "        # Avg Pool 1\n",
    "        x_ap1 = self.avgpool(x_dcv1) # [32, 28, 32, 32]\n",
    "\n",
    "        # Concat depwise conv with maxpool\n",
    "        x_cc2 = torch.concat((x_dcv1, x_mp), dim=1) # [ 32, 44, 64, 64]\n",
    "        \n",
    "        # Depthwise conv 2\n",
    "        x_dcv2 = self.depthwise_conv_2(x_cc2) #[32, 32, 32, 32]\n",
    "\n",
    "        # Avg Pool 2\n",
    "        x_ap2 = self.avgpool(x_dcv2) # [32, 32, 16, 16]\n",
    "\n",
    "        # Concat depthwise conv 2 with avgpool 1\n",
    "        x_cc3 = torch.concat((x_dcv2, x_ap1), dim=1) # [32, 60, 32, 32]\n",
    "\n",
    "        # Deptwise conv 3\n",
    "        x_dcv3 = self.depthwise_conv_3(x_cc3) # [32, 32, 16, 16]\n",
    "\n",
    "        # Avg Pool 3\n",
    "        x_ap3 = self.avgpool(x_dcv3) # [32,32,8,8]\n",
    "\n",
    "        # Concat depthwise conv3 with avgpool 2\n",
    "        x_cc4 = torch.concat((x_dcv3, x_ap2), dim=1) # [32, 64, 16, 16]\n",
    "\n",
    "        # Depthwise conv 4\n",
    "        x_dcv4 = self.depthwise_conv_4(x_cc4) # [32,32,8,8]\n",
    "\n",
    "        # Concat depwise conv4 with avgpool 3\n",
    "        x_cc5 = torch.concat((x_dcv4, x_ap3), dim=1) # [32, 64, 8, 8]\n",
    "\n",
    "        # Depwise conv 5\n",
    "        x_dcv5 = self.depthwise_conv_5(x_cc5) # [32, 32, 8, 8]\n",
    "\n",
    "        # Concat Depthwise conv4, AvgPool 3 and x_cc5\n",
    "        x = torch.cat((x_dcv4, x_ap3, x_dcv5), dim=1) # [32, 96, 8, 8]\n",
    "\n",
    "        x = self.pool_block(x) # [32, 96, 1, 1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Train and Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Focus(nn.Module): \n",
    "    # Focus wh information into c-space \n",
    "    def __init__(self, c1, c2, k=1, s=1, p=0, g=1):  # ch_in, ch_out, kernel, stride, padding, groups \n",
    "        super(Focus, self).__init__() \n",
    "        self.conv = nn.Conv2d(c1 * 4, c2, k, s, p, groups=g) \n",
    "        # self.contract = Contract(gain=2) \n",
    "\n",
    "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2) \n",
    "        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
    "\n",
    "class CodnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.focus = Focus(3, 16)\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, 2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 28, 2, 2, groups=4), # kernel= 3x3?\n",
    "            nn.Conv2d(28, 28, 1),\n",
    "            nn.BatchNorm2d(28),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.depthwise_conv_2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(44),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(44, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(60),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(60, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 2, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=1)\n",
    "        )\n",
    "        self.depthwise_conv_5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Conv2d(64, 128, 1),\n",
    "            nn.Conv2d(128, 128, 5, 1, groups=4), # kernel=7x7?\n",
    "            nn.Conv2d(128, 32, 1, padding=2)\n",
    "        )\n",
    "        self.pool_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Hardswish(),\n",
    "            nn.AvgPool2d(8)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(96, 512),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.5)\n",
    "        )\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial Conv\n",
    "        x_cv = self.conv_1(x) # [32, 16, 128, 128\n",
    "\n",
    "        # Focus\n",
    "        x_f= self.focus(x) # [32, 16, 128, 628]\n",
    "\n",
    "        # Max Pool\n",
    "        x_mp = self.maxpool(x_cv) # [32, 16, 64, 64]\n",
    "\n",
    "        # Concat initial Conv with Focus\n",
    "        x_cc1 = torch.concat((x_cv, x_f), dim=1) #[32, 32, 128, 128]\n",
    "\n",
    "        # Depthwise conv 1\n",
    "        x_dcv1= self.depthwise_conv_1(x_cc1) # [32, 28, 64, 64]\n",
    "\n",
    "        # Avg Pool 1\n",
    "        x_ap1 = self.avgpool(x_dcv1) # [32, 28, 32, 32]\n",
    "\n",
    "        # Concat depwise conv with maxpool\n",
    "        x_cc2 = torch.concat((x_dcv1, x_mp), dim=1) # [ 32, 44, 64, 64]\n",
    "        \n",
    "        # Depthwise conv 2\n",
    "        x_dcv2 = self.depthwise_conv_2(x_cc2) #[32, 32, 32, 32]\n",
    "\n",
    "        # Avg Pool 2\n",
    "        x_ap2 = self.avgpool(x_dcv2) # [32, 32, 16, 16]\n",
    "\n",
    "        # Concat depthwise conv 2 with avgpool 1\n",
    "        x_cc3 = torch.concat((x_dcv2, x_ap1), dim=1) # [32, 60, 32, 32]\n",
    "\n",
    "        # Deptwise conv 3\n",
    "        x_dcv3 = self.depthwise_conv_3(x_cc3) # [32, 32, 16, 16]\n",
    "\n",
    "        # Avg Pool 3\n",
    "        x_ap3 = self.avgpool(x_dcv3) # [32,32,8,8]\n",
    "\n",
    "        # Concat depthwise conv3 with avgpool 2\n",
    "        x_cc4 = torch.concat((x_dcv3, x_ap2), dim=1) # [32, 64, 16, 16]\n",
    "\n",
    "        # Depthwise conv 4\n",
    "        x_dcv4 = self.depthwise_conv_4(x_cc4) # [32,32,8,8]\n",
    "\n",
    "        # Concat depwise conv4 with avgpool 3\n",
    "        x_cc5 = torch.concat((x_dcv4, x_ap3), dim=1) # [32, 64, 8, 8]\n",
    "\n",
    "        # Depwise conv 5\n",
    "        x_dcv5 = self.depthwise_conv_5(x_cc5) # [32, 32, 8, 8]\n",
    "\n",
    "        # Concat Depthwise conv4, AvgPool 3 and x_cc5\n",
    "        x = torch.cat((x_dcv4, x_ap3, x_dcv5), dim=1) # [32, 96, 8, 8]\n",
    "\n",
    "        x = self.pool_block(x) # [32, 96, 1, 1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "        break\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model_0, train_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Convert to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/training.py\n",
    "import torch\n",
    "import tor  chmetrics\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Convert to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/save_model.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    target_dir = os.path.join(\"../Covid-Classificaton\", target_dir)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train, Evaluate & Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Run this before running below code\n",
    "sys.path.append(\"/Users/zhengyaosiah/Documents/Code/Classification-Covid/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "\n",
    "list(data_transforms.items())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, save_model, training\n",
    "import covid_aid, squeeze_net, efficient_cnn, gru_cnn\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE={\n",
    "    \"GRUCNN\": 0.0001,\n",
    "               \"EfficientCNN\": 0.001,\n",
    "               \"CovidAid\": 0.001, \n",
    "               \"SqueezeNet\": 0.0001}\n",
    "EPOCHS = 15\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = {\n",
    "                    \"GRUCNN\": transforms.Compose([transforms.Resize(224),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"EfficientCNN\": transforms.Compose([transforms.Resize(150),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "# Models\n",
    "models = {\n",
    "    \"GRUCNN\": gru_cnn.GRUCNN().to(device), \n",
    "          \"EfficientCNN\": efficient_cnn.EFFICIENT_CNN().to(device),\n",
    "            \"CovidAid\": covid_aid.CovidAidModel().to(device), \n",
    "            \"SqueezeNet\": squeeze_net.SqueezeNet().to(device)}\n",
    "\n",
    "optimizers =  {\n",
    "    \"GRUCNN\": torch.optim.Adam(models[\"GRUCNN\"].parameters(),lr=LEARNING_RATE[\"GRUCNN\"]),\n",
    "    \"EfficientCNN\": torch.optim.Adam(models[\"EfficientCNN\"].parameters(), lr=LEARNING_RATE[\"EfficientCNN\"]),\n",
    "    \"CovidAid\": torch.optim.SGD(models[\"CovidAid\"].parameters(), lr=LEARNING_RATE[\"CovidAid\"]),\n",
    "    \"SqueezeNet\": torch.optim.Adam(models[\"SqueezeNet\"].parameters(), lr=LEARNING_RATE[\"SqueezeNet\"])\n",
    "}\n",
    "\n",
    "\n",
    "def training_loop(data_transforms, models):\n",
    "    for model_name in models:\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        # DATA\n",
    "        train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms[model_name], BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # optimizer = torch.optim.SGD(models[model_name].parameters(), lr=LEARNING_RATE[model_name])\n",
    "        optimizer = optimizers[model_name]\n",
    "\n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "\n",
    "        # Train models\n",
    "        model_results = training.train(models[model_name], train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "        # End timer\n",
    "        end_time = timer()\n",
    "\n",
    "        # Misc\n",
    "        print(f\"{model_name} training time: {end_time-start_time:.2f} seconds\")\n",
    "        pytorch_total_params = sum(p.numel() for p in models[model_name].parameters())\n",
    "        print(f\"Number of parameters: {pytorch_total_params}\")\n",
    "\n",
    "        # Save Model\n",
    "        save_model.save_model(models[model_name], target_dir='models', model_name=f'{model_name}.pt')\n",
    "\n",
    "        # plot graph\n",
    "        fig, ax = plt.subplots()\n",
    "        x_axis = [i for i in range(EPOCHS)]\n",
    "        y_axis = model_results[\"train_loss\"]\n",
    "        ax.plot(x_axis, y_axis)\n",
    "        ax.set(xlabel=\"Epochs\", ylabel=\"Train Loss\", title=model_name)\n",
    "        fig.savefig(f\"testloss_{model_name}\")\n",
    "\n",
    "training_loop(data_transforms, models)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Convert to Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/executable.py\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, save_model, training\n",
    "import covid_aid, squeeze_net, efficient_cnn, gru_cnn\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE={\n",
    "    \"GRUCNN\": 0.0001,\n",
    "               \"EfficientCNN\": 0.001,\n",
    "               \"CovidAid\": 0.001, \n",
    "               \"SqueezeNet\": 0.0001}\n",
    "EPOCHS = 15\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = {\n",
    "                    \"GRUCNN\": transforms.Compose([transforms.Resize(224),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"EfficientCNN\": transforms.Compose([transforms.Resize(150),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "# Models\n",
    "models = {\n",
    "    \"GRUCNN\": gru_cnn.GRUCNN().to(device), \n",
    "          \"EfficientCNN\": efficient_cnn.EFFICIENT_CNN().to(device),\n",
    "            \"CovidAid\": covid_aid.CovidAidModel().to(device), \n",
    "            \"SqueezeNet\": squeeze_net.SqueezeNet().to(device)}\n",
    "\n",
    "optimizers =  {\n",
    "    \"GRUCNN\": torch.optim.Adam(models[\"GRUCNN\"].parameters(),lr=LEARNING_RATE[\"GRUCNN\"]),\n",
    "    \"EfficientCNN\": torch.optim.Adam(models[\"EfficientCNN\"].parameters(), lr=LEARNING_RATE[\"EfficientCNN\"]),\n",
    "    \"CovidAid\": torch.optim.SGD(models[\"CovidAid\"].parameters(), lr=LEARNING_RATE[\"CovidAid\"]),\n",
    "    \"SqueezeNet\": torch.optim.Adam(models[\"SqueezeNet\"].parameters(), lr=LEARNING_RATE[\"SqueezeNet\"])\n",
    "}\n",
    "\n",
    "\n",
    "def training_loop(data_transforms, models):\n",
    "    for model_name in models:\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        # DATA\n",
    "        train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms[model_name], BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        # optimizer = torch.optim.SGD(models[model_name].parameters(), lr=LEARNING_RATE[model_name])\n",
    "        optimizer = optimizers[model_name]\n",
    "\n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "\n",
    "        # Train models\n",
    "        model_results = training.train(models[model_name], train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "        # End timer\n",
    "        end_time = timer()\n",
    "\n",
    "        # Misc\n",
    "        print(f\"{model_name} training time: {end_time-start_time:.2f} seconds\")\n",
    "        pytorch_total_params = sum(p.numel() for p in models[model_name].parameters())\n",
    "        print(f\"Number of parameters: {pytorch_total_params}\")\n",
    "\n",
    "        # Save Model\n",
    "        save_model.save_model(models[model_name], target_dir='models', model_name=f'{model_name}.pt')\n",
    "\n",
    "        # plot graph\n",
    "        fig, ax = plt.subplots()\n",
    "        x_axis = [i for i in range(EPOCHS)]\n",
    "        y_axis = model_results[\"train_loss\"]\n",
    "        ax.plot(x_axis, y_axis)\n",
    "        ax.set(xlabel=\"Epochs\", ylabel=\"Train Loss\", title=model_name)\n",
    "        fig.savefig(f\"testloss_{model_name}\")\n",
    "\n",
    "training_loop(data_transforms, models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
