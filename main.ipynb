{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dir = \"covid-dataset\"\n",
    "class_names = ['COVID', 'Normal', 'Viral Pneumonia']\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Make directories with each class name\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Train and test ratio\n",
    "split_ratio = 0.8\n",
    "covid_train_split = int(len(os.listdir(\"covid-dataset/COVID/images/\")) * split_ratio)\n",
    "normal_train_split = int(len(os.listdir(\"covid-dataset/Normal/images/\")) * split_ratio)\n",
    "pneumonia_train_split = int(len(os.listdir(\"covid-dataset/Viral Pneumonia/images/\")) * split_ratio)\n",
    "\n",
    "# Shuffle directories\n",
    "for class_name in class_names:\n",
    "    random.shuffle(os.listdir(dataset_dir + \"/\" + class_name + \"/images\"))\n",
    "\n",
    "# Move images into train and test\n",
    "covid_src_images = os.listdir(\"covid-dataset/COVID/images/\")\n",
    "train_covid_images = covid_src_images[:covid_train_split]\n",
    "test_covid_images = covid_src_images[covid_train_split:]\n",
    "for image in train_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/COVID\")\n",
    "for image in test_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/COVID\")\n",
    "\n",
    "normal_src_images = os.listdir(\"covid-dataset/Normal/images/\")\n",
    "train_normal_images = normal_src_images[:normal_train_split]\n",
    "test_normal_images = normal_src_images[normal_train_split:]\n",
    "for image in train_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Normal\")\n",
    "for image in test_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Normal\")\n",
    "\n",
    "pneumonia_src_images = os.listdir(\"covid-dataset/Viral Pneumonia/images/\")\n",
    "train_pneumonia_images = pneumonia_src_images[:pneumonia_train_split]\n",
    "test_pneumonia_images = pneumonia_src_images[pneumonia_train_split:]\n",
    "for image in train_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Viral Pneumonia\")\n",
    "for image in test_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Viral Pneumonia\")\n",
    "\n",
    "print(\"Files moved successfully!\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'Normal', 'Viral Pneumonia']\n",
      "Train data: Dataset ImageFolder\n",
      "    Number of datapoints: 12121\n",
      "    Root location: covid-dataset\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) \n",
      " Test data: Dataset ImageFolder\n",
      "    Number of datapoints: 3032\n",
      "    Root location: covid-dataset\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f\"Train data: {train_data} \\n Test data: {test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Turn it into a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scripts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/data_setup.py\n",
    "\"\"\"\n",
    "Putting data into Imagefolder and Dataloader\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "def create_dataloaders(train_dir, test_dir, transform, batch_size, num_workers):\n",
    "    train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_data,batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Model (CovidAid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image.shape: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m img_single, label_single \u001b[39m=\u001b[39m img_batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), label_batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSingle image.shape: \u001b[39m\u001b[39m{\u001b[39;00mimg_single\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m model_0\u001b[39m.\u001b[39meval()\n\u001b[0;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m     11\u001b[0m     pred \u001b[39m=\u001b[39m model_0(img_single\u001b[39m.\u001b[39mto(device))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_0' is not defined"
     ]
    }
   ],
   "source": [
    "# Dummy forward pass to test if it works\n",
    "# batches of images and its label\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# testibg model to see if it works\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image.shape: {img_single.shape}\")\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model_0(img_single.to(device))\n",
    "    \n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 CovidAid Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/covid_aid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/covid_aid.py\n",
    "\"\"\"\n",
    "Contains code about CovidAid Model. Original paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9418407\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Squeeze Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([96, 55, 55])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8.0663e-01,  8.0663e-01,  7.3614e-01,  ...,  4.8381e-01,\n",
       "           7.0762e-01,  6.3100e-01],\n",
       "         [ 8.0663e-01,  8.0663e-01,  7.1580e-01,  ...,  6.6817e-01,\n",
       "           7.2797e-01,  7.2797e-01],\n",
       "         [ 6.9625e-01,  8.2464e-01,  8.2464e-01,  ...,  6.8575e-01,\n",
       "           7.2797e-01,  7.2797e-01],\n",
       "         ...,\n",
       "         [ 6.5404e-01,  7.3019e-01,  7.7358e-01,  ...,  6.5305e-01,\n",
       "           6.2561e-01,  6.2090e-01],\n",
       "         [ 7.3302e-01,  7.0545e-01,  7.7358e-01,  ...,  6.5305e-01,\n",
       "           6.2561e-01,  6.2090e-01],\n",
       "         [ 5.3952e-01,  5.3952e-01,  3.7404e-01,  ...,  6.6434e-01,\n",
       "           5.6052e-01,  4.5004e-01]],\n",
       "\n",
       "        [[ 3.7472e-01,  2.8119e-01,  3.1155e-01,  ...,  3.2233e-01,\n",
       "           3.0552e-01,  3.0201e-01],\n",
       "         [ 3.0673e-01,  3.3790e-01,  2.1017e-01,  ...,  3.1381e-01,\n",
       "           2.2797e-01,  3.0201e-01],\n",
       "         [ 1.1941e-01,  2.5139e-01,  3.1782e-01,  ...,  3.3702e-01,\n",
       "           4.9994e-01,  3.2326e-01],\n",
       "         ...,\n",
       "         [ 2.5118e-01,  3.5618e-01,  4.0229e-01,  ...,  3.3045e-01,\n",
       "           2.9797e-01,  2.9797e-01],\n",
       "         [ 2.6828e-01,  5.1578e-01,  5.1578e-01,  ...,  3.0723e-01,\n",
       "           3.0723e-01,  1.7960e-01],\n",
       "         [ 3.6426e-01,  5.1578e-01,  5.1578e-01,  ...,  3.0521e-01,\n",
       "           3.4140e-01,  3.4140e-01]],\n",
       "\n",
       "        [[ 3.6334e-02,  8.1863e-02,  8.5925e-02,  ..., -2.2919e-02,\n",
       "           4.2399e-01,  1.2161e-01],\n",
       "         [-2.2661e-02, -1.8308e-02,  8.5382e-02,  ..., -1.5503e-01,\n",
       "           4.2399e-01,  2.0446e-01],\n",
       "         [-1.3334e-01, -6.3917e-02, -2.8063e-02,  ...,  2.3366e-01,\n",
       "           6.6702e-02,  2.0446e-01],\n",
       "         ...,\n",
       "         [ 5.9035e-02, -9.0215e-02, -4.7086e-02,  ...,  1.6307e-01,\n",
       "           1.6307e-01,  1.2512e-01],\n",
       "         [ 6.1104e-02,  2.3171e-01,  2.3171e-01,  ...,  1.6307e-01,\n",
       "           1.6307e-01,  1.2501e-01],\n",
       "         [ 1.5255e-01,  2.3171e-01,  2.3171e-01,  ...,  9.7211e-02,\n",
       "           1.2501e-01,  1.2501e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.7580e-01, -2.5161e-02,  9.6782e-02,  ...,  2.0829e-02,\n",
       "          -7.5896e-02, -2.8704e-03],\n",
       "         [-3.8124e-02, -3.8124e-02, -2.1031e-01,  ...,  2.0829e-02,\n",
       "           5.6744e-02,  1.4297e-01],\n",
       "         [ 6.6256e-03, -2.0562e-02, -1.6886e-01,  ..., -1.9021e-01,\n",
       "           5.6744e-02,  2.3774e-02],\n",
       "         ...,\n",
       "         [ 1.4116e-01, -2.8673e-01, -1.7524e-01,  ..., -1.3326e-01,\n",
       "           1.9248e-03, -8.0299e-02],\n",
       "         [-4.8146e-04, -5.6735e-02, -5.6087e-02,  ..., -1.1609e-01,\n",
       "          -2.7833e-02, -1.0833e-01],\n",
       "         [ 1.2785e-01, -1.0081e-01, -5.6087e-02,  ..., -8.7852e-02,\n",
       "          -6.0227e-02,  8.6951e-02]],\n",
       "\n",
       "        [[ 4.2368e-01,  3.7878e-01,  3.7878e-01,  ...,  2.8440e-01,\n",
       "           3.0408e-01,  3.0408e-01],\n",
       "         [ 3.2308e-01,  2.4796e-01,  2.8059e-01,  ...,  3.0699e-01,\n",
       "           3.9737e-01,  3.9737e-01],\n",
       "         [ 4.5346e-01,  3.2649e-01,  2.8059e-01,  ...,  2.4180e-01,\n",
       "           3.9737e-01,  3.9737e-01],\n",
       "         ...,\n",
       "         [ 4.9811e-01,  5.2714e-01,  4.6960e-01,  ...,  3.4021e-01,\n",
       "           3.3640e-01,  2.4838e-01],\n",
       "         [ 4.2873e-01,  5.2714e-01,  4.6960e-01,  ...,  2.7215e-01,\n",
       "           2.6450e-01,  1.5843e-01],\n",
       "         [ 4.8913e-01,  2.6395e-01,  4.5967e-01,  ...,  4.0124e-01,\n",
       "           7.6120e-01,  7.6120e-01]],\n",
       "\n",
       "        [[ 6.2387e-01,  6.4799e-01,  7.3902e-01,  ...,  5.9996e-01,\n",
       "           6.7725e-01,  5.9758e-01],\n",
       "         [ 8.5702e-01,  8.0288e-01,  7.2042e-01,  ...,  7.2148e-01,\n",
       "           8.7165e-01,  8.7165e-01],\n",
       "         [ 8.8518e-01,  8.8518e-01,  7.6527e-01,  ...,  9.0979e-01,\n",
       "           7.0851e-01,  7.8901e-01],\n",
       "         ...,\n",
       "         [ 8.1900e-01,  7.5321e-01,  8.1442e-01,  ...,  7.1550e-01,\n",
       "           7.8281e-01,  7.8281e-01],\n",
       "         [ 6.9821e-01,  7.2624e-01,  8.1442e-01,  ...,  7.1550e-01,\n",
       "           8.4326e-01,  7.4715e-01],\n",
       "         [ 7.3022e-01,  7.1632e-01,  7.7176e-01,  ...,  9.0569e-01,\n",
       "           8.4920e-01,  7.8984e-01]]], grad_fn=<MaxPool2DWithIndicesBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "test_img = torch.rand(3, 224,224)\n",
    "print(test_img.shape)\n",
    "model = SqueezeNet()\n",
    "model(test_img) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Train and Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "        break\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model_0, train_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Convert to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/training.py\n",
    "import torch\n",
    "import torchmetrics\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Convert to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/save_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/save_model.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    target_dir = os.path.join(\"../Covid-Classificaton\", target_dir)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train, Evaluate & Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m     29\u001b[0m \u001b[39m# Train models\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m model_0_results \u001b[39m=\u001b[39m train(model_0, optimizer, loss_fn, EPOCHS, device)\n\u001b[0;32m     32\u001b[0m \u001b[39m# End timer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS= 4 #os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# MODELS\n",
    "model_0 = CovidAidModel().to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy_fn = MulticlassAccuracy(num_classes=3)\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Start timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train models\n",
    "model_0_results = train(model_0, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "# End timer\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "# Save Model\n",
    "save_model(model_0, target_dir='models', model_name='CovidAid.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Convert to Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/executable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/executable.py\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, covid_aid, save_model, training\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 5\n",
    "print(f\"Learning Rate: {LEARNING_RATE} | Number of Epochs: {EPOCHS}\")\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# DATA\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms, BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "# MODELS\n",
    "model_0 = covid_aid.CovidAidModel().to(device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Start timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train models\n",
    "model_0_results = training.train(model_0, train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "# End timer\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "# Save Model\n",
    "save_model.save_model(model_0, target_dir='models', model_name='CovidAid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
