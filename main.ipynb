{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'covid-dataset/COVID/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Train and test ratio\u001b[39;00m\n\u001b[1;32m     14\u001b[0m split_ratio \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m\n\u001b[0;32m---> 15\u001b[0m covid_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39mcovid-dataset/COVID/images/\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m*\u001b[39m split_ratio)\n\u001b[1;32m     16\u001b[0m normal_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mcovid-dataset/Normal/images/\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m*\u001b[39m split_ratio)\n\u001b[1;32m     17\u001b[0m pneumonia_train_split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39m\"\u001b[39m\u001b[39mcovid-dataset/Viral Pneumonia/images/\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m*\u001b[39m split_ratio)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'covid-dataset/COVID/images/'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_dir = \"covid-dataset\"\n",
    "class_names = ['COVID', 'Normal', 'Viral Pneumonia']\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "test_dir = os.path.join(dataset_dir, \"test\")\n",
    "\n",
    "# Make directories with each class name\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "for class_name in class_names:\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Train and test ratio\n",
    "split_ratio = 0.8\n",
    "covid_train_split = int(len(os.listdir(\"covid-dataset/COVID/images/\")) * split_ratio)\n",
    "normal_train_split = int(len(os.listdir(\"covid-dataset/Normal/images/\")) * split_ratio)\n",
    "pneumonia_train_split = int(len(os.listdir(\"covid-dataset/Viral Pneumonia/images/\")) * split_ratio)\n",
    "\n",
    "# Shuffle directories\n",
    "for class_name in class_names:\n",
    "    random.shuffle(os.listdir(dataset_dir + \"/\" + class_name + \"/images\"))\n",
    "\n",
    "# Move images into train and test\n",
    "covid_src_images = os.listdir(\"covid-dataset/COVID/images/\")\n",
    "train_covid_images = covid_src_images[:covid_train_split]\n",
    "test_covid_images = covid_src_images[covid_train_split:]\n",
    "for image in train_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/COVID\")\n",
    "for image in test_covid_images:\n",
    "    image_path = os.path.join(\"covid-dataset/COVID/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/COVID\")\n",
    "\n",
    "normal_src_images = os.listdir(\"covid-dataset/Normal/images/\")\n",
    "train_normal_images = normal_src_images[:normal_train_split]\n",
    "test_normal_images = normal_src_images[normal_train_split:]\n",
    "for image in train_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Normal\")\n",
    "for image in test_normal_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Normal/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Normal\")\n",
    "\n",
    "pneumonia_src_images = os.listdir(\"covid-dataset/Viral Pneumonia/images/\")\n",
    "train_pneumonia_images = pneumonia_src_images[:pneumonia_train_split]\n",
    "test_pneumonia_images = pneumonia_src_images[pneumonia_train_split:]\n",
    "for image in train_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/train/Viral Pneumonia\")\n",
    "for image in test_pneumonia_images:\n",
    "    image_path = os.path.join(\"covid-dataset/Viral Pneumonia/images/\", image)\n",
    "    shutil.move(image_path, \"covid-dataset/test/Viral Pneumonia\")\n",
    "\n",
    "print(\"Files moved successfully!\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=os.cpu_count()\n",
    "LEARNING_RATE=0.01\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'Normal', 'Viral Pneumonia']\n",
      "Train data: Dataset ImageFolder\n",
      "    Number of datapoints: 12121\n",
      "    Root location: covid-dataset/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) \n",
      " Test data: Dataset ImageFolder\n",
      "    Number of datapoints: 3032\n",
      "    Root location: covid-dataset/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=data_transforms)\n",
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f\"Train data: {train_data} \\n Test data: {test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Turn it into a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"scripts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/data_setup.py\n",
    "\"\"\"\n",
    "Putting data into Imagefolder and Dataloader\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "def create_dataloaders(train_dir, test_dir, transform, batch_size, num_workers):\n",
    "    train_data = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_data,batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Model (CovidAid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        print(x.shape)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x) # [32,3]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image.shape: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Dummy forward pass to test if it works\n",
    "# batches of images and its label\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# testibg model to see if it works\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image.shape: {img_single.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 256, 256])\n",
      "Output shape: torch.Size([1, 3])\n",
      "torch.Size([1, 8, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2785, -0.1801, -0.7465]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CovidAidModel()\n",
    "print(f\"Output shape: {model(img_single).shape}\")\n",
    "model(img_single)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 CovidAid Script mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/covid_aid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/covid_aid.py\n",
    "\"\"\"\n",
    "Contains code about CovidAid Model. Original paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9418407\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CovidAidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.covid_aid_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.covid_aid_7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.covid_aid_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.covid_aid_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.covid_aid_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(363, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.covid_aid_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_2(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_3(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_block_4(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        x = self.covid_aid_3(x)\n",
    "        x = self.covid_aid_4(x)\n",
    "        x = self.covid_aid_5(x)\n",
    "        x = self.covid_aid_6(x)\n",
    "        x = self.covid_aid_7(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Squeeze Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Squeeze Net takes input of 224 instead of 256, so have to scale it accordingly\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    \"\"\"\n",
    "    s1x1: number of filters in squeeze layer (all 1x1)\n",
    "    e1x1: number of 1x1 filters in expand layer\n",
    "    e3x3: number of 3x3 filters in expand layer\n",
    "    s1x1 > (e1x1 + e3x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, s1x1,e1x1,e3x3,):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=s1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.expand_e1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.expand_e3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        y = self.expand_e1x1(x) # 64\n",
    "        z = self.expand_e3x3(x) # 64\n",
    "        return torch.cat((y, z),dim=1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=2)\n",
    "        self.conv_10 = nn.Conv2d(in_channels=512, out_channels=3, kernel_size=1, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.maxpool_8 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.fireModule_2 = FireModule(96, 16, 64, 64)\n",
    "        self.fireModule_3 = FireModule(128, 16, 64, 64)\n",
    "        self.fireModule_4 = FireModule(128, 32, 128, 128)\n",
    "        self.fireModule_5 = FireModule(256, 32, 128, 128)\n",
    "        self.fireModule_6 = FireModule(256, 48, 192, 192)\n",
    "        self.fireModule_7 = FireModule(384, 48, 192, 192)   \n",
    "        self.fireModule_8 = FireModule(384, 64, 256, 256)\n",
    "        self.fireModule_9 = FireModule(512, 64, 256, 256)\n",
    "        self.avgpool_10 = nn.AvgPool2d(kernel_size=17, stride=1, padding=0)\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x) # [32, 96, 111, 111]\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool_1(x) # [32, 96, 55, 55]\n",
    "        x = self.fireModule_2(x) # [32, 128,55,55]\n",
    "        x = self.fireModule_3(x) # [128,55,55]\n",
    "        x = self.fireModule_4(x) # [256,55,55]\n",
    "        x = self.maxpool_4(x) # [256,27,27]\n",
    "        x = self.fireModule_5(x) # [256,27,27]\n",
    "        x = self.fireModule_6(x) # [384, 27, 27]\n",
    "        x = self.fireModule_7(x) # [384, 27, 27]\n",
    "        x = self.fireModule_8(x) # [512, 27, 27]\n",
    "        x = self.maxpool_8(x) # [512, 13, 13]\n",
    "        x = self.fireModule_9(x) # [512, 13, 13]\n",
    "        x = self.conv_10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool_10(x) # [3,1,1] \n",
    "        # x = self.softmax(x)\n",
    "        x = torch.squeeze(x, dim=3)\n",
    "        x= torch.squeeze(x, dim=2)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0241, 0.0000, 0.0010]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.rand(1,3, 224, 224)\n",
    "\n",
    "\n",
    "model = SqueezeNet()\n",
    "print(f\"Output shape: {model(test_img).shape}\")\n",
    "model(test_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 SqueezeNet Script Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/squeeze_net.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/squeeze_net.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    \"\"\"\n",
    "    s1x1: number of filters in squeeze layer (all 1x1)\n",
    "    e1x1: number of 1x1 filters in expand layer\n",
    "    e3x3: number of 3x3 filters in expand layer\n",
    "    s1x1 > (e1x1 + e3x3)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, s1x1,e1x1,e3x3,):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=s1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.expand_e1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e1x1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.expand_e3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=s1x1, out_channels=e3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        y = self.expand_e1x1(x) # 64\n",
    "        z = self.expand_e3x3(x) # 64\n",
    "        return torch.cat((y, z),dim=1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2, padding=2)\n",
    "        self.conv_10 = nn.Conv2d(in_channels=512, out_channels=3, kernel_size=1, stride=1, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.maxpool_4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.maxpool_8 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        self.fireModule_2 = FireModule(96, 16, 64, 64)\n",
    "        self.fireModule_3 = FireModule(128, 16, 64, 64)\n",
    "        self.fireModule_4 = FireModule(128, 32, 128, 128)\n",
    "        self.fireModule_5 = FireModule(256, 32, 128, 128)\n",
    "        self.fireModule_6 = FireModule(256, 48, 192, 192)\n",
    "        self.fireModule_7 = FireModule(384, 48, 192, 192)   \n",
    "        self.fireModule_8 = FireModule(384, 64, 256, 256)\n",
    "        self.fireModule_9 = FireModule(512, 64, 256, 256)\n",
    "        self.avgpool_10 = nn.AvgPool2d(kernel_size=17, stride=1, padding=0)\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x) # [32, 96, 111, 111]\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool_1(x) # [32, 96, 55, 55]\n",
    "        x = self.fireModule_2(x) # [32, 128,55,55]\n",
    "        x = self.fireModule_3(x) # [128,55,55]\n",
    "        x = self.fireModule_4(x) # [256,55,55]\n",
    "        x = self.maxpool_4(x) # [256,27,27]\n",
    "        x = self.fireModule_5(x) # [256,27,27]\n",
    "        x = self.fireModule_6(x) # [384, 27, 27]\n",
    "        x = self.fireModule_7(x) # [384, 27, 27]\n",
    "        x = self.fireModule_8(x) # [512, 27, 27]\n",
    "        x = self.maxpool_8(x) # [512, 13, 13]\n",
    "        x = self.fireModule_9(x) # [512, 13, 13]\n",
    "        x = self.conv_10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.avgpool_10(x) # [3,1,1] \n",
    "        # x = self.softmax(x)\n",
    "        x = torch.squeeze(x, dim=3)\n",
    "        x= torch.squeeze(x, dim=2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Deep GRU-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep GRU-CNN input 224 x 224\n",
    "https://ieeexplore.ieee.org/abstract/document/9423965\n",
    "https://discuss.pytorch.org/t/input-shape-to-gru-layer/171318\n",
    "\"\"\"\n",
    "\n",
    "class GRUCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 64, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.conv_block_5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(512*7*7*BATCH_SIZE, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        # self.gru = nn.GRU(input_size=3584, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.gru = nn.GRU(input_size=1, hidden_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        x = self.conv_block_5(x) # torch.Size([32, 512, 7, 7])\n",
    "        \n",
    "        # reshape the tensor \n",
    "        x = x.view(1, -1, 1) # [1, 512*7*7*BATCH_SIZE, 1]\n",
    "        x, _ = self.gru(x) # _ represents hidden state\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.unsqueeze(x,dim=0)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengyaosiah/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3138, 0.3381, 0.3481]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = torch.randn(32, 3, 224,224)\n",
    "model = GRUCNN()\n",
    "print(model(test_img).shape)\n",
    "model(test_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Efficient CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S1568494622007050#fig3\n",
    "Efficient_CNN: https://www.hindawi.com/journals/complexity/2021/6621607/fig4/\n",
    "\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class EFFICIENT_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.block_6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 1),\n",
    "            nn.MaxPool2d(1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "        self.dense_1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_3 = nn.Sequential(\n",
    "            nn.Linear(256, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        x = self.block_6(x)\n",
    "        x = torch.flatten(x)\n",
    "\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        x = torch.unsqueeze(x, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 3])\n",
      "tensor([[0.5128, 0.5091, 0.5085]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_img = torch.randn(1,3,150, 150)\n",
    "\n",
    "model = EFFICIENT_CNN()\n",
    "print(f\"Output shape: {model(test_img).shape}\")\n",
    "print(model(test_img))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Train and Test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "        break\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step(model_0, train_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Convert to a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/training.py\n",
    "import torch\n",
    "import torchmetrics\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Forward\n",
    "        y_pred = model(X) # returns shape [32,3]\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy across batch\n",
    "        y_pred_class = torch.argmax(y_pred,dim=1)\n",
    "        train_acc += accuracy_fn(y_pred_class, y)\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # loss\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # accuracy across batch\n",
    "            test_pred_label = torch.argmax(y_pred,dim=1)\n",
    "            test_acc += accuracy_fn(test_pred_label, y)\n",
    "\n",
    "\n",
    "    # get average loss and acc per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f} | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Convert to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/save_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/save_model.py\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    # Create directory to save models\n",
    "    target_dir = os.path.join(\"../Covid-Classificaton\", target_dir)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pt\")\n",
    "    model_saved_path = target_dir + '/' + model_name\n",
    "\n",
    "    # save model\n",
    "    print(f\"Saved model to: {model_saved_path}\")\n",
    "    torch.save(model.state_dict(), model_saved_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train, Evaluate & Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Run this before running below code\n",
    "sys.path.append(\"/Users/zhengyaosiah/Documents/Code/Classification-Covid/scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SqueezeNet',\n",
       " Compose(\n",
       "     Resize(size=224, interpolation=bilinear, max_size=None, antialias=warn)\n",
       "     ToTensor()\n",
       " ))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transforms = {\"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "\n",
    "list(data_transforms.items())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, save_model, training\n",
    "import covid_aid, squeeze_net\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE={\"CovidAid\": 0.01, \"SqueezeNet\": 0.04}\n",
    "EPOCHS = 5\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = {\"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "# Models\n",
    "models = {\"CovidAid\": covid_aid.CovidAidModel().to(device), \"SqueezeNet\": squeeze_net.SqueezeNet().to(device)}\n",
    "\n",
    "def training_loop(data_transforms, models):\n",
    "    for model_name in models:\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        # DATA\n",
    "        train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms[model_name], BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(models[model_name].parameters(), lr=LEARNING_RATE[model_name])\n",
    "\n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "\n",
    "        # Train models\n",
    "        model_results = training.train(models[model_name], train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "        # End timer\n",
    "        end_time = timer()\n",
    "        print(f\"{model_name} training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "        # Save Model\n",
    "        save_model.save_model(models[model_name], target_dir='models', model_name=f'{model_name}.pt')\n",
    "\n",
    "training_loop(data_transforms, models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Convert to Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/executable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/executable.py\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import data_setup, save_model, training\n",
    "import covid_aid, squeeze_net\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED=42\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS=4 #os.cpu_count()\n",
    "LEARNING_RATE={\"CovidAid\": 0.01, \"SqueezeNet\": 0.04}\n",
    "EPOCHS = 5\n",
    "\n",
    "# Instantiniate seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Varaibles\n",
    "train_dir = \"../covid-dataset/train/\"\n",
    "test_dir = \"../covid-dataset/test/\"\n",
    "data_transforms = {\"CovidAid\":transforms.Compose([transforms.Resize(256),\n",
    "                                                  transforms.ToTensor()]),\n",
    "                    \"SqueezeNet\": transforms.Compose([transforms.Resize(224),\n",
    "                                                    transforms.ToTensor()])}\n",
    "# Models\n",
    "models = {\"CovidAid\": covid_aid.CovidAidModel().to(device), \"SqueezeNet\": squeeze_net.SqueezeNet().to(device)}\n",
    "\n",
    "def training_loop(data_transforms, models):\n",
    "    for model_name in models:\n",
    "        print(f\"Model Name: {model_name}\")\n",
    "        # DATA\n",
    "        train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir, test_dir, data_transforms[model_name], BATCH_SIZE, NUM_WORKERS)\n",
    "\n",
    "        # Loss Function and Optimizer\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(models[model_name].parameters(), lr=LEARNING_RATE[model_name])\n",
    "\n",
    "        # Start timer\n",
    "        start_time = timer()\n",
    "\n",
    "        # Train models\n",
    "        model_results = training.train(models[model_name], train_dataloader, test_dataloader, optimizer, loss_fn, EPOCHS, device)\n",
    "\n",
    "        # End timer\n",
    "        end_time = timer()\n",
    "        print(f\"{model_name} training time: {end_time-start_time:.2f} seconds\")\n",
    "\n",
    "        # Save Model\n",
    "        save_model.save_model(models[model_name], target_dir='models', model_name=f'{model_name}.pt')\n",
    "\n",
    "training_loop(data_transforms, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
